{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI: A hybrid deep model for fake news detection\n",
    "\n",
    "## Temporal event modeling based on LSTM\n",
    "### Glossary\n",
    "    - Event(article) : Instance which is True/False or Rumor/NonRumor\n",
    "    - Message : Temporal behavior on a certain event.\n",
    "        - Each message arrives at a certain timestamp.\n",
    "        - Each message has auxiliary information such as context, user_info.\n",
    "    - Dataset\n",
    "        - Weibo : 4,664 events, (2,313/2,351 Rumor/NonRumor), 3,805,656 posts, 2,746,818 users\n",
    "                Avg. # of posts/event : 816\n",
    "                Max # of posts/event : 59,318\n",
    "                Min # of posts/event : 10\n",
    "        - Tweet : 992 events, (498/494 Rumor/NonRumor), 1,101,985 posts,  491,229 users\n",
    "                Avg. # of posts/event : 1,111\n",
    "                Max # of posts/event : 62,827\n",
    "                Min # of posts/event : 10\n",
    "\n",
    "### Description\n",
    "    - input: (d) then (d,text,user)\n",
    "    - output: (d)\n",
    "    - end result: embedding vector h\n",
    "    \n",
    "    - how do we capture text (tf-idf, word2vec, tweet2vec)\n",
    "    - how do we capture user-info (user name/id, degree?, activity level?)\n",
    "    - how do we capture *group* dynamics....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# events : 992\n",
      "# users : 233719\n",
      "# messages : 592391\n",
      "Avg. time length : 7138609.0 sec\t1982.94694444 hours\n",
      "Avg. # messages : 597.168346774\n",
      "Max # messages : 39167\n",
      "Min # messages : 4\n",
      "Avg. messages / each user : 2.53462919146\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load dataset\n",
    "'''\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "def get_stats(dict_):\n",
    "    nb_messages = []\n",
    "    eid_list = []\n",
    "    user_set = set()\n",
    "    list_lengths = []\n",
    "    for eid, messages in dict_.items():\n",
    "        nb_messages.append(len(messages['timestamps']))\n",
    "        eid_list.append(eid)\n",
    "\n",
    "        user_set.update(messages['uid'])\n",
    "        ts = np.array(messages['timestamps'], dtype=np.float32)\n",
    "        list_lengths.append(ts[-1]-ts[0])\n",
    "        \n",
    "    return nb_messages, eid_list, user_set, list_lengths\n",
    "\n",
    "\n",
    "def create_dataset(dict_, eid, threshold=90, resolution='day',\n",
    "                   read_text=False, embeddings_index=None, stopwords=None,\n",
    "                   doc2vec_model=None, user_feature=None, user2ind=None, read_user=False, task='regression',\n",
    "                   cutoff=50, return_useridx=True):\n",
    "    messages = dict_[eid]\n",
    "    ts = np.array(messages['timestamps'], dtype=np.int32)\n",
    "    try:\n",
    "        user_list = messages['uid'].tolist()\n",
    "    except:\n",
    "        user_list = messages['uid']\n",
    "    if read_text:\n",
    "        text_seq = np.array(messages['text'])\n",
    "    else:\n",
    "        text_seq = None\n",
    "        \n",
    "    if read_user:\n",
    "        XX, XX_uidx = get_features(eid, ts, threshold=threshold, resolution=resolution, read_text=read_text,\n",
    "                              text_seq=text_seq, embeddings_index=embeddings_index, stopwords=stopwords,\n",
    "                              doc2vec_model=doc2vec_model, read_user=read_user,\n",
    "                              user_feature=user_feature, user2ind=user2ind, user_list=user_list,\n",
    "                              cutoff=cutoff, return_useridx=return_useridx)\n",
    "    else:\n",
    "        XX = get_features(eid, ts, threshold=threshold, resolution=resolution, read_text=read_text,\n",
    "                              text_seq=text_seq, embeddings_index=embeddings_index, stopwords=stopwords,\n",
    "                              doc2vec_model=doc2vec_model, read_user=read_user,\n",
    "                              user_feature=user_feature, user2ind=user2ind, user_list=user_list,\n",
    "                              cutoff=cutoff, return_useridx=return_useridx)\n",
    "\n",
    "#     print(eid, XX.shape, X.shape)\n",
    "    if task==\"regression\":\n",
    "        X = XX[:-1,:]   # (nb_sample, 2+)\n",
    "        y = XX[1:,:2]\n",
    "#         y = XX[1:,1]\n",
    "        if len(y.shape)==1:\n",
    "            return X, y.reshape(-1,1)\n",
    "        elif len(y.shape)==2:\n",
    "            return X, y\n",
    "    elif task==\"classification\":\n",
    "        X = XX   # (nb_sample, 2+)\n",
    "        y = int(messages['label'])\n",
    "        if return_useridx:\n",
    "            return X, XX_uidx, y\n",
    "        else:\n",
    "            return X, y\n",
    "\n",
    "\n",
    "def get_features(eid, timestamps, threshold=90, resolution='day', sep=False, read_text=False,\n",
    "                 text_seq=None, embeddings_index=None, stopwords=None, read_user=False,\n",
    "                 doc2vec_model=None, user_feature=None, user2ind=None, user_list=None,\n",
    "                 cutoff=50, return_useridx=True):\n",
    "    '''\n",
    "    timestamps\n",
    "        : relative timestamps since the first tweet\n",
    "        : it should be sorted.\n",
    "        : unit = second\n",
    "    unit of threshold and resolution should be matched.\n",
    "    '''\n",
    "    ts = timestamps\n",
    "    if resolution=='day':\n",
    "        binsize = 3600*24\n",
    "    elif resolution=='hour':\n",
    "        binsize = 3600\n",
    "    elif resolution=='minute':\n",
    "        binsize = 60\n",
    "    cnt, bins = np.histogram(ts, bins=range(0,threshold*binsize,binsize))\n",
    "    \n",
    "    nonzero_bins_ind = np.nonzero(cnt)[0]\n",
    "    nonzero_bins = bins[nonzero_bins_ind]\n",
    "    \n",
    "    hist = cnt[nonzero_bins_ind]\n",
    "    inv = nonzero_bins_ind[1:]-nonzero_bins_ind[:-1]\n",
    "    intervals = np.insert(inv,0,0)\n",
    "    ### Cutoff sequence\n",
    "#     cutoff = 50\n",
    "    if len(hist)>cutoff:\n",
    "        hist = hist[:cutoff]\n",
    "        intervals = intervals[:cutoff]\n",
    "        nonzero_bins = nonzero_bins[:cutoff]\n",
    "\n",
    "    ### user feature   \n",
    "    if read_user:\n",
    "        X_useridx = []\n",
    "        for bid, bin_left in enumerate(nonzero_bins):\n",
    "            bin_userlist = []\n",
    "            bin_right = bin_left + binsize\n",
    "            try:\n",
    "                del temp\n",
    "            except:\n",
    "                pass\n",
    "            # Collecting text to make doc\n",
    "            for tid, t in enumerate(ts):\n",
    "                if t<bin_left:\n",
    "                    continue\n",
    "                elif t>=bin_right:\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "                uid = user2ind[user_list[tid]]\n",
    "                bin_userlist.append(user_list[tid])\n",
    "                coef = user_feature[uid,:].reshape(1,-1)   # (1,n_components)\n",
    "                try:\n",
    "                    temp = np.concatenate((temp, coef), axis=0)\n",
    "                except:\n",
    "                    temp = coef\n",
    "\n",
    "            X_user_bin = np.mean(temp, axis=0).reshape(1,-1)\n",
    "\n",
    "            try:\n",
    "                X_user = np.concatenate((X_user, X_user_bin), axis=0)\n",
    "            except:\n",
    "                X_user = X_user_bin\n",
    "            X_useridx.append(bin_userlist)\n",
    "            \n",
    "    ### text feature\n",
    "    if read_text:\n",
    "        text_matrix = get_doc2vec(doc2vec_model, eid, nonzero_bins)\n",
    "    \n",
    "    if sep:\n",
    "        if read_text:\n",
    "            return hist, intervals, X_user, text_matrix\n",
    "        else:\n",
    "            return hist, intervals, X_user\n",
    "    else:\n",
    "        if read_text and read_user:\n",
    "            if return_useridx:\n",
    "                return np.hstack([hist.reshape(-1,1), intervals.reshape(-1,1), X_user, text_matrix]), X_useridx\n",
    "            else:\n",
    "                return np.hstack([hist.reshape(-1,1), intervals.reshape(-1,1), X_user, text_matrix])\n",
    "        elif read_text or read_user:\n",
    "            if read_text:\n",
    "                return np.hstack([hist.reshape(-1,1), intervals.reshape(-1,1), text_matrix])\n",
    "            elif read_user:\n",
    "                if return_useridx:\n",
    "                    return np.hstack([hist.reshape(-1,1), intervals.reshape(-1,1), X_user]), X_useridx\n",
    "                else:\n",
    "                    return np.hstack([hist.reshape(-1,1), intervals.reshape(-1,1), X_user])\n",
    "        else:\n",
    "            return np.hstack([hist.reshape(-1,1), intervals.reshape(-1,1)])\n",
    "    \n",
    "def get_doc2vec(doc2vec_model, eid, nonzero_bins):\n",
    "    for bid, bin_left in enumerate(nonzero_bins):\n",
    "        if isinstance(eid, int):\n",
    "            eid_str = str(eid)\n",
    "        else:\n",
    "            eid_str = eid\n",
    "        tag = eid_str+'_'+str(bid)\n",
    "        temp = doc2vec_model.docvecs[tag]  # (300,)\n",
    "        temp = temp.reshape(1,-1)\n",
    "        try:\n",
    "            X_text = np.concatenate((X_text, temp), axis=0)\n",
    "        except:\n",
    "            X_text = temp\n",
    "    return X_text\n",
    "\n",
    "\n",
    "### Load dataset ###\n",
    "dict_ = pickle.load(open('/drive2/sungyong/data/fake_reviews/twitter/tweet_dict_ids.pkl','rb'))\n",
    "    \n",
    "# Get statistics\n",
    "# tweet eid is 'E741' or 'TM1211'\n",
    "# eid is a number, 3906982031327232\n",
    "nb_messages, eid_list, user_set, list_lengths = get_stats(dict_)    \n",
    "\n",
    "print(\"# events : {}\".format(len(eid_list)))\n",
    "print(\"# users : {}\".format(len(user_set)))\n",
    "print(\"# messages : {}\".format(np.sum(nb_messages)))\n",
    "print(\"Avg. time length : {} sec\\t{} hours\".format(np.mean(list_lengths),np.mean(list_lengths)/3600))\n",
    "print(\"Avg. # messages : {}\".format(np.mean(nb_messages)))\n",
    "print(\"Max # messages : {}\".format(np.max(nb_messages)))\n",
    "print(\"Min # messages : {}\".format(np.min(nb_messages)))\n",
    "print(\"Avg. messages / each user : {}\".format(np.sum(nb_messages)/len(user_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get two sets of user features\n",
    "- `u_sample` : list of tuples (user_ID, # appearances). Top-K users are sampled. \n",
    "- `u_pop` : list of tuples (user_ID, # appearances). All users are sampled. \n",
    "- `user_feature` ($U$$\\Sigma$) : comes from user x event matrix, $M = U$ $\\Sigma$$V^T$.\n",
    "    - $M[i,j]$ : 1 if i-th user from `u_pop` interacts with j-th event. Otherwise, 0.\n",
    "- `user_feature_sub` ($U'$$\\Sigma'$) : comes from user x user matrix, $M' = U'$ $\\Sigma'$$V'^T$.\n",
    "    - $M' = PP^T$\n",
    "    - $P[i,j]$ : 1 if i-th user from `u_sample` interacts with j-th event. Otherwise, 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_sample for most common 20000 users is obtained.\n",
      "u_pop for all 233719 users is obtained.\n",
      "# users in u_sample : 20000\n",
      "# users : 233719\n",
      "# events : 992\n",
      "932 events have at least one user in u_sample\n",
      "60 events have no user in u_sample\n",
      "992 events have at least one user in u_sample\n",
      "0 events have no user in u_sample\n",
      "992 events have at least one user in u_sample\n",
      "0 events have no user in u_sample\n",
      "matrix_sub shape : (20000, 932)\n",
      "Sparsity : 0.00855305793991\n",
      "matrix_main shape : (233719, 992)\n",
      "Sparsity : 0.00182664383712\n",
      "user_feature shape : (233719, 100)\n",
      "user_feature_sub shape : (20000, 100)\n",
      "Loading is Done.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_Usample(dict_, most_common=50):\n",
    "    '''Get U_sample who are most_common.'''\n",
    "    u_sample = []\n",
    "    cnt = Counter()\n",
    "    for ii, (eid, value) in enumerate(dict_.items()):\n",
    "        users = value['uid']\n",
    "        cnt.update(users)\n",
    "    return cnt.most_common(most_common)    # [(user_id, #occur in all events), ...]\n",
    "\n",
    "def get_user_in_event(dict_, eid, u_sample):\n",
    "    '''Get users who acts on a given event, eid'''\n",
    "    value = dict_[eid]\n",
    "    cnt = Counter(value['uid'])\n",
    "    users = set(value['uid'])\n",
    "    user_in_event = []\n",
    "    for uid, nb_occur in u_sample:\n",
    "        if uid in users:\n",
    "#             user_in_event.append((uid, nb_occur))    # [(user_id, #occur in all events), (user_id, #occur in all events), ...]\n",
    "            user_in_event.append((uid, cnt[uid]))    # [(user_id, #occur in eid), (user_id, #occur in eid), ...]\n",
    "    return user_in_event    # [(user_id, #occur in eid), (user_id, #occur in eid), ...]\n",
    "\n",
    "threshold = 20000\n",
    "u_sample = get_Usample(dict_, most_common=threshold)\n",
    "u_pop = get_Usample(dict_, most_common=len(user_set))\n",
    "print(\"u_sample for most common {} users is obtained.\".format(threshold))\n",
    "print(\"u_pop for all {} users is obtained.\".format(len(user_set)))\n",
    "\n",
    "\n",
    "'''\n",
    "Here are Two user-event matrices.\n",
    "    1) matrix_main : (all user - all event) relation\n",
    "        It has #occurrences of a user(row) in an event(col)\n",
    "        It is very sparse.\n",
    "        It is decomposed with smaller K.\n",
    "    2) matrix_sub : (u_sample - eid_sample) relation\n",
    "        It has #occurrences of a user(row) in an event(col)\n",
    "        It is denser.\n",
    "        It is decomposed with larger K. (usually)\n",
    "'''\n",
    "user_sample2ind = {}\n",
    "for ii, (uid, nb_occur) in enumerate(u_sample):\n",
    "    user_sample2ind[uid] = ii\n",
    "print(\"# users in u_sample : {}\".format(len(user_sample2ind)))\n",
    "user2ind = {}\n",
    "for ii, uid in enumerate(user_set):\n",
    "    user2ind[uid] = ii\n",
    "print(\"# users : {}\".format(len(user2ind)))\n",
    "eid2ind = {}\n",
    "for ii, eid in enumerate(eid_list):\n",
    "    eid2ind[eid] = ii\n",
    "print(\"# events : {}\".format(len(eid2ind)))\n",
    "\n",
    "\n",
    "'''\n",
    "User Features\n",
    "    Generate userid-eid matrix and Decompose it\n",
    "    Truncated SVD\n",
    "'''\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def get_user_event_matrix(dict_, u_sample, user2ind, binary=False):\n",
    "    '''Get (user,event) matrix.\n",
    "    This matrix will be decomposed by TruncatedSVD (or else?)\n",
    "    Only users in u_sample are considered.\n",
    "    '''\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    jj = 0\n",
    "    eid2ind = {}\n",
    "    for ii, (eid, value) in enumerate(dict_.items()):\n",
    "        user_in_event = get_user_in_event(dict_, eid, u_sample)\n",
    "        if len(user_in_event)==0:\n",
    "            # No user in u_sample appears in this eid event.\n",
    "            continue\n",
    "        else:\n",
    "            eid2ind[eid] = jj\n",
    "#             eind = eid2ind[eid]\n",
    "        for uid, nb_occur in user_in_event:\n",
    "            uind = user2ind[uid]\n",
    "            col.append(jj)\n",
    "            row.append(uind)\n",
    "            if binary:\n",
    "                data.append(1)    # Binary matrix\n",
    "            else:\n",
    "                data.append(nb_occur)\n",
    "        jj+=1\n",
    "    print(\"{} events have at least one user in u_sample\".format(jj))\n",
    "    print(\"{} events have no user in u_sample\".format(len(dict_)-jj))\n",
    "    return csr_matrix((data, (row, col)), shape=(len(user2ind), len(eid2ind))), eid2ind\n",
    "    \n",
    "\n",
    "matrix_sub, eid_sample2ind = get_user_event_matrix(dict_, u_sample, user_sample2ind, binary=True)\n",
    "matrix_main, eid_main2ind = get_user_event_matrix(dict_, u_pop, user2ind, binary=True)\n",
    "matrix_main_cnt, eid_main_cnt2ind = get_user_event_matrix(dict_, u_pop, user2ind, binary=False)\n",
    "print(\"matrix_sub shape : {}\".format(matrix_sub.shape))\n",
    "print(\"Sparsity : {}\".format(matrix_sub.count_nonzero()/(matrix_sub.shape[0]*matrix_sub.shape[1])))\n",
    "print(\"matrix_main shape : {}\".format(matrix_main.shape))\n",
    "print(\"Sparsity : {}\".format(matrix_main.count_nonzero()/(matrix_main.shape[0]*matrix_main.shape[1])))\n",
    "\n",
    "# from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "RELOAD = True\n",
    "if RELOAD:\n",
    "    ### Load matrix_main\n",
    "    u_main = np.load(open('matrix/tweet_u_main.npy','rb'))\n",
    "    sigma_main = np.load(open('matrix/tweet_sigma_main.npy','rb'))\n",
    "    vt_main = np.load(open('matrix/tweet_vt_main.npy','rb'))\n",
    "\n",
    "    user_feature = u_main.dot(np.diag(sigma_main))\n",
    "    nb_feature_main = 20     # 10 for weibo, 20 for tweet\n",
    "    print(\"user_feature shape : {}\".format(user_feature.shape))\n",
    "\n",
    "    ### Load matrix_sub\n",
    "    u_sub = np.load(open('matrix/tweet_u_sub.npy','rb'))\n",
    "    sigma_sub = np.load(open('matrix/tweet_sigma_sub.npy','rb'))\n",
    "    vt_sib = np.load(open('matrix/tweet_vt_sub.npy','rb'))\n",
    "\n",
    "    user_feature_sub = u_sub.dot(np.diag(sigma_sub))\n",
    "    nb_feature_sub = 50\n",
    "    print(\"user_feature_sub shape : {}\".format(user_feature_sub.shape))\n",
    "    print(\"Loading is Done.\")\n",
    "else:\n",
    "    nb_feature_main = 20     # 10 for weibo, 20 for tweet\n",
    "    n_iter = 7    # 15 for weibo, 7 for tweet\n",
    "    u_main, sigma_main, vt_main = randomized_svd(matrix_main, n_components=100,\n",
    "                                                 n_iter=n_iter, random_state=42)  # random_state=42\n",
    "    user_feature = u_main.dot(np.diag(sigma_main))\n",
    "    print(\"user_feature shape : {}\".format(user_feature.shape))\n",
    "\n",
    "    nb_feature_sub = 50\n",
    "    matrix_sub = matrix_sub.dot(matrix_sub.transpose())\n",
    "    matrix_sub_array = matrix_sub.toarray()\n",
    "    u_sub, sigma_sub, vt_sub = randomized_svd(matrix_sub, n_components=100,\n",
    "                                              n_iter=n_iter, random_state=42)  # random_state=42\n",
    "    user_feature_sub = u_sub.dot(np.diag(sigma_sub))\n",
    "    print(\"user_feature_sub shape : {}\".format(user_feature_sub.shape))\n",
    "    print(\"SVD is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba, re\n",
    "\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "threshold = 90*24\n",
    "resolution = 'hour'\n",
    "sentences = []\n",
    "\n",
    "chinese_stopwords = '、 。 〃 〄 々 〆 〇 〈〉 《 》 「 」 『 』 【】 〒 〓 〔 〕 〖 〗 〘〙 〚 〛 〛 〜 〝 〞 〟，'.decode('utf-8')\n",
    "rx = '[' + re.escape(''.join(chinese_stopwords.split())) + ']'\n",
    "\n",
    "for ii, eid in enumerate(eid_list):\n",
    "    if ii%100==0:\n",
    "        print(\"{}th event {} is processing...\".format(ii+1, eid))\n",
    "    messages = dict_[eid]\n",
    "    ts = np.array(messages['timestamps'], dtype=np.int32)\n",
    "    text_seq = np.array(messages['text'])\n",
    "    \n",
    "    if resolution=='day':\n",
    "        binsize = 3600*24\n",
    "    elif resolution=='hour':\n",
    "        binsize = 3600\n",
    "    elif resolution=='minute':\n",
    "        binsize = 60\n",
    "    cnt, bins = np.histogram(ts, bins=range(0,threshold*binsize,binsize))\n",
    "    \n",
    "    nonzero_bins_ind = np.nonzero(cnt)[0]\n",
    "    nonzero_bins = bins[nonzero_bins_ind]\n",
    "    print(ii, eid, len(nonzero_bins))\n",
    "    hist = cnt[nonzero_bins_ind]\n",
    "    inv = nonzero_bins_ind[1:]-nonzero_bins_ind[:-1]\n",
    "    intervals = np.insert(inv,0,0)\n",
    "\n",
    "    for bid, bin_left in enumerate(nonzero_bins):\n",
    "        bin_right = bin_left + binsize\n",
    "        try:\n",
    "            del doc\n",
    "        except:\n",
    "            pass\n",
    "        # Collecting text to make doc\n",
    "        for tid, t in enumerate(ts):\n",
    "            if t<bin_left:\n",
    "                continue\n",
    "            elif t>=bin_right:\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "            string = text_seq[tid]\n",
    "            string = re.sub(r\"http\\S+\", \"\", string)\n",
    "            string = re.sub(\"[?!.,:;()'@#$%^&*-=+/\\[\\[\\]\\]]\", ' ', string) # !.,:;()'@#$%^&*-_{}=+/\\\"\n",
    "            try:\n",
    "                doc += string\n",
    "            except:\n",
    "                doc = string\n",
    "        if isinstance(eid, int):\n",
    "            eid_str = str(eid)\n",
    "        else:\n",
    "            eid_str = eid\n",
    "        sentences.append(LabeledSentence(utils.to_unicode(doc).split(), [eid_str+'_%s' % bid]))\n",
    "    \n",
    "\n",
    "print(\"length of sentences : {}\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_vectorizer is loaded.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "try:\n",
    "    doc_vectorizer = Doc2Vec.load('./doc2vec_model/tweet_doc2vec_thrd9024_dim100.model')\n",
    "    print(\"doc_vectorizer is loaded.\")\n",
    "except:\n",
    "    doc_vectorizer = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)\n",
    "    doc_vectorizer.build_vocab(sentences)\n",
    "    print(\"build_vocab is done.\")\n",
    "\n",
    "    for epoch in range(10):\n",
    "        print(epoch)\n",
    "        doc_vectorizer.train(sentences)\n",
    "    print(\"doc2vec training is done.\")\n",
    "\n",
    "    # doc_vectorizer.docvecs['TM71_0'].shape\n",
    "    # doc_vectorizer.save('./doc2vec_model/weibo_doc2vec_thrd9024_dim100.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyong/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing... 1/992  shape:(27, 122)\n",
      "processing... 101/992  shape:(17, 122)\n",
      "processing... 201/992  shape:(50, 122)\n",
      "processing... 301/992  shape:(50, 122)\n",
      "processing... 401/992  shape:(50, 122)\n",
      "processing... 501/992  shape:(8, 122)\n",
      "processing... 601/992  shape:(34, 122)\n",
      "processing... 701/992  shape:(29, 122)\n",
      "processing... 801/992  shape:(50, 122)\n",
      "processing... 901/992  shape:(50, 122)\n",
      "Dataset are created.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "def get_user_feature_in_event(dict_, eid, u_sample, user_feature_sub, user_sample2ind):\n",
    "    '''Get user_feature_sub matrix for event eid'''\n",
    "    user_in_event = get_user_in_event(dict_, eid, u_sample)\n",
    "    nb_feature = user_feature_sub.shape[1]\n",
    "    \n",
    "    for uid, nb_occur in user_in_event:\n",
    "        uind = user_sample2ind[uid]\n",
    "        feature_vec = user_feature_sub[uind,:].reshape(1,-1)\n",
    "        try:\n",
    "            ret_matrix = np.concatenate((ret_matrix, feature_vec), axis=0)\n",
    "        except:\n",
    "            ret_matrix = feature_vec\n",
    "    try:\n",
    "        return ret_matrix\n",
    "    except:\n",
    "        ### if user_in_event is empty\n",
    "        return np.zeros((1,nb_feature))\n",
    "    \n",
    "### Building Model\n",
    "LOAD_MODEL = False\n",
    "task = \"classification\"  #\"classification\"\n",
    "\n",
    "scaler_dict = {}\n",
    "nb_rumor = 0\n",
    "noerr_eid_list = set()\n",
    "burnin = 5 if task==\"regression\" else 0\n",
    "\n",
    "### Create dataset ###\n",
    "X_dict = {}\n",
    "X_uidx_dict = {}\n",
    "subX_dict = {}\n",
    "y_dict = {}\n",
    "read_text = True\n",
    "read_user = True\n",
    "\n",
    "rumor_user = []\n",
    "nonrumor_user = []\n",
    "eid_train, eid_test, _, _ = train_test_split(eid_list, range(len(eid_list)),\n",
    "                                             test_size=0.2, random_state=3)\n",
    "\n",
    "### To make same sets \n",
    "import pickle\n",
    "eid_train = pickle.load(open('./pickle/tweet_eid_train.pkl','r'))\n",
    "eid_test = pickle.load(open('./pickle/tweet_eid_test.pkl','r'))\n",
    "\n",
    "# embeddings_index = None\n",
    "# doc_vectorizer = None\n",
    "for ii, eid in enumerate(eid_list):\n",
    "    if read_user:\n",
    "        X, X_uidx, y = create_dataset(dict_, eid, threshold=90*24, resolution='hour',\n",
    "                                 read_text=read_text, embeddings_index=None, stopwords=None,\n",
    "                                 doc2vec_model=doc_vectorizer, user_feature=user_feature[:,:nb_feature_main], \n",
    "                                 user2ind=user2ind, read_user=read_user, task=task, cutoff=50,\n",
    "                                 return_useridx=True)\n",
    "    else:\n",
    "        X, y = create_dataset(dict_, eid, threshold=90*24, resolution='hour',\n",
    "                                 read_text=read_text, embeddings_index=None, stopwords=None,\n",
    "                                 doc2vec_model=doc_vectorizer, user_feature=user_feature[:,:nb_feature_main], \n",
    "                                 user2ind=user2ind, read_user=read_user, task=task, cutoff=50,\n",
    "                                 return_useridx=False)\n",
    "    if ii%100==0:\n",
    "        print(\"processing... {}/{}  shape:{}\".format(ii+1, len(eid_list), X.shape))\n",
    "        \n",
    "    label = int(dict_[eid]['label'])\n",
    "    if label==0:\n",
    "        nonrumor_user.extend(dict_[eid]['uid'])\n",
    "    elif label==1:\n",
    "        rumor_user.extend(dict_[eid]['uid'])\n",
    "#     user_ids.update(dict_[eid]['to_user_id'])\n",
    "    X = X.astype(np.float32)\n",
    "    if X.shape[0]<=2*burnin:  # ignore length<=1 sequence\n",
    "        continue\n",
    "\n",
    "    X_dict[eid] = X\n",
    "    if read_user:\n",
    "        X_uidx_dict[eid] = X_uidx\n",
    "    subX_dict[eid] = get_user_feature_in_event(dict_, eid, u_sample, \n",
    "                                               user_feature_sub[:,:nb_feature_sub], user_sample2ind)\n",
    "    y_dict[eid] = y\n",
    "\n",
    "    try:\n",
    "        scaler_dict[eid]\n",
    "    except:\n",
    "        scaler_hist = MinMaxScaler(feature_range=(0,1))\n",
    "        scaler_hist.fit(X[:,0].reshape(-1,1))\n",
    "        scaler_interval = MinMaxScaler(feature_range=(0,1))\n",
    "        scaler_interval.fit(X[:,1].reshape(-1,1))\n",
    "        scaler_dict[eid] = (scaler_hist, scaler_interval)\n",
    "print(\"Dataset are created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSI model\n",
    "- Python : 2.7.x\n",
    "- Keras : 1.2.1\n",
    "- Theano : 0.9.0b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "matrix_main is used for LSTM input.\n",
    "matrix_sub is used for the scoring module.\n",
    "'''\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense, Input, Dropout, Lambda, LSTM, Embedding, Conv1D, TimeDistributed, merge\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "acc=0\n",
    "\n",
    "nb_users = len(user2ind)\n",
    "nb_events = len(eid2ind)\n",
    "nb_features = 2+20+100    # (#temporal, #user, #doc)\n",
    "dim_hidden = 50\n",
    "\n",
    "##### Main part #####\n",
    "inputs = Input(shape=(None, nb_features))\n",
    "emb_out = TimeDistributed(Dense(100, activation='tanh'))(inputs)    # W_e\n",
    "emb_out = Dropout(0.2)(emb_out)\n",
    "rnn_out = LSTM(dim_hidden, activation='tanh', return_sequences=False)(emb_out)    #(None, dim_hidden)\n",
    "rnn_out = Dense(100, activation='tanh')(rnn_out)     # (None, 100) W_r\n",
    "rnn_out = Dropout(0.2)(rnn_out)\n",
    "\n",
    "\n",
    "##### Sub part #####\n",
    "nb_score = 1\n",
    "nb_expand = 100\n",
    "sub_input = Input(shape=(None, nb_feature_sub))\n",
    "user_vec = TimeDistributed(Dense(nb_expand, activation='tanh',\n",
    "                                 W_regularizer=regularizers.l2(0.01)))(sub_input)   # (None, None, nb_expand)\n",
    "sub_h = TimeDistributed(Dense(nb_score, activation='sigmoid'))(user_vec)    # (None, None, nb_score)\n",
    "z = Lambda(lambda x: K.mean(x, axis=1), output_shape=lambda s: (s[0], s[2]))(sub_h)    #(None, nb_score)\n",
    "\n",
    "##### Concatenate #####\n",
    "out1 = Dense(1, activation='sigmoid')(rnn_out)\n",
    "concat_out = merge([out1, z], mode='sum')\n",
    "# concat_out = merge([rnn_out, z], mode='concat', concat_axis=1)\n",
    "# concat_out = concatenate([rnn_out, z], axis=1)\n",
    "\n",
    "##### Classifier #####\n",
    "# outputs = Dense(1, activation='sigmoid')(concat_out)\n",
    "# outputs = Dense(1, activation='sigmoid')(concat_out)\n",
    "outputs = concat_out\n",
    "\n",
    "\n",
    "##### Model #####\n",
    "hvector = Model(input=[inputs, sub_input], output=concat_out)\n",
    "zscore = Model(input=sub_input, output=sub_h)\n",
    "model = Model(input=[inputs, sub_input], output=outputs)\n",
    "uvector = Model(input=sub_input, output=user_vec)\n",
    "# model = Model(input=inputs, output=outputs)\n",
    "\n",
    "\n",
    "##### Compile #####\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "if task==\"regression\":\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='mean_squared_error')\n",
    "elif task==\"classification\":\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='binary_crossentropy')\n",
    "print(\"Model is compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, confusion_matrix\n",
    "from keras.models import load_model\n",
    "\n",
    "def sigmoid_array(x):                                        \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "### Training... ###\n",
    "# acc = 0\n",
    "nb_epoch = 30\n",
    "if task==\"regression\":\n",
    "    eid_train = eid_list\n",
    "    eid_test = []\n",
    "    \n",
    "for ep in range(nb_epoch+1):\n",
    "    print(\"{} epoch!!!!!!!!\".format(ep))\n",
    "    ##### Looping for eid_train #####\n",
    "    losses = []\n",
    "    for ii, eid in enumerate(eid_train):\n",
    "        if X.shape[0]<=2*burnin:  # ignore length<=1 sequence\n",
    "            continue\n",
    "\n",
    "        X = X_dict[eid]\n",
    "        X = X.astype(np.float32)\n",
    "        y = y_dict[eid]\n",
    "\n",
    "        label = int(dict_[eid]['label'])\n",
    "        if task==\"classification\":\n",
    "            assert(label==y)\n",
    "\n",
    "        noerr_eid_list.add(eid)\n",
    "\n",
    "        sh = scaler_dict[eid][0]\n",
    "        si = scaler_dict[eid][1]\n",
    "        \n",
    "        ##### Main input #####\n",
    "        trainX = X\n",
    "        ##### Sub input #####\n",
    "        sub_trainX = subX_dict[eid]\n",
    "    \n",
    "        if task==\"regression\":\n",
    "            ### TODO : if we want to predict more features, add here.\n",
    "            if y.shape[1]>1:\n",
    "                trainY = np.hstack([sh.transform(y[:,0].reshape(-1,1)),\n",
    "                                    si.transform(y[:,1].reshape(-1,1))])\n",
    "            else:\n",
    "                trainY = si.transform(y)\n",
    "            dim_output = trainY.shape\n",
    "            \n",
    "        elif task==\"classification\":\n",
    "            trainY = y\n",
    "            dim_output = 1\n",
    "        \n",
    "        if ep%50==0 and ii%1000==0:\n",
    "            h = model.fit([trainX[np.newaxis,:,:], sub_trainX[np.newaxis,:,:]], np.array([trainY]), \n",
    "                          batch_size=1, nb_epoch=1, verbose=2)\n",
    "        else:\n",
    "            h = model.fit([trainX[np.newaxis,:,:], sub_trainX[np.newaxis,:,:]], np.array([trainY]), \n",
    "                          batch_size=1, nb_epoch=1, verbose=0)\n",
    "        losses.append(h.history['loss'][0])\n",
    "    print(\"%% mean loss : {}\".format(np.mean(losses)))\n",
    "\n",
    "    ### Evaluation ###\n",
    "    preds = []\n",
    "    rmses = []\n",
    "    y_test = []\n",
    "    for ii, eid in enumerate(eid_test):\n",
    "        if X.shape[0]<=2*burnin:  # ignore length<=1 sequence\n",
    "            continue\n",
    "\n",
    "        X = X_dict[eid]\n",
    "        X = X.astype(np.float32)\n",
    "        y = y_dict[eid]\n",
    "\n",
    "        testX = X\n",
    "        sub_testX = subX_dict[eid]\n",
    "        \n",
    "        if task==\"classification\":\n",
    "            y_test.append(int(dict_[eid]['label']))\n",
    "\n",
    "            pred = model.predict([np.array([testX]), np.array([sub_testX])], verbose=0)\n",
    "            preds.append(pred[0,0])\n",
    "            \n",
    "        elif task==\"regression\":\n",
    "            predict_y = model.predict(np.array([testX]), verbose=0)\n",
    "            \n",
    "            sh = scaler_dict[eid][0]\n",
    "            si = scaler_dict[eid][1]\n",
    "\n",
    "            if predict_y.shape[2]==1:\n",
    "                predict_y = np.hstack([sh.inverse_transform(predict_y[0,burnin:,0].reshape(-1,1))])\n",
    "            elif predict_y.shape[2]==2:\n",
    "                predict_y = np.hstack([sh.inverse_transform(predict_y[0,burnin:,0].reshape(-1,1)),\n",
    "                                       si.inverse_transform(predict_y[0,burnin:,1].reshape(-1,1))])\n",
    "            elif predict_y.shape[2]>2:\n",
    "                predict_y = np.hstack([sh.inverse_transform(predict_y[0,burnin:,0].reshape(-1,1)),\n",
    "                                       si.inverse_transform(predict_y[0,burnin:,1].reshape(-1,1)),\n",
    "                                       predict_y[0,burnin:,2:]])\n",
    "            nb_features = predict_y.shape[1]\n",
    "            rmse = np.sqrt(np.mean((predict_y[:-1,:] - trainX[burnin+1:,:nb_features])**2))\n",
    "            rmses.append(rmse)\n",
    "\n",
    "    if task==\"classification\":\n",
    "        preds = np.array(preds)\n",
    "        preds = preds>0.5\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "        print(\"%%% Test results {} samples %%%\".format(len(y_test)))\n",
    "        print(\"accuracy: {}\".format((tp+tn)/(tp+tn+fp+fn)))\n",
    "        print(\"precision : {:.4f} / {:.4f}\".format(tp/(tp+fp), tn/(fn+tn)))\n",
    "        print(\"recall : {:.4f} / {:.4f}\".format(tp/(tp+fn), tn/(fp+tn)))\n",
    "        print(\"F1 score : {:.4f} / {:.4f}\".format(2*tp/(2*tp+fp+fn), 2*tn/(2*tn+fp+fn)))\n",
    "\n",
    "\n",
    "    elif task==\"regression\":\n",
    "        print(\"%%% Test results {} samples\".format(len(rmses)))\n",
    "        print(\"mean rmse : {}\".format(np.mean(rmses)))\n",
    "        \n",
    "    if acc < (tp+tn)/(tp+tn+fp+fn):\n",
    "        acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "        print(\"%%%%%%%%%% Save model\\t acc:{} %%%%%%%%%%%%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
