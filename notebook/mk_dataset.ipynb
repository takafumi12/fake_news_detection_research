{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f2d8886-7e6d-44ab-8692-b4e35ccc0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.charfilter import *\n",
    "from janome.tokenfilter import *\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "\n",
    "from dateutil import tz\n",
    "UTC = tz.gettz(\"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4e7e04-d2fa-4d05-bb56-0ecd79d62f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def dump(value, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(value, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b9397e-4ebe-4e22-8932-a057f854d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_data_path = '../data/scraping_data/'\n",
    "dataset_path = '../data/dataset/'\n",
    "fij_tweet_data = load(scraping_data_path+'fij_news_data/tweet_data.pkl')\n",
    "infact_tweet_data = load(scraping_data_path+'infact_news_data/tweet_data.pkl')\n",
    "nikkei_tweet_data = load(scraping_data_path+'nikkei_news_data/tweet_data.pkl')\n",
    "fij_fake_tweet_text_dict = load(scraping_data_path+'fij_news_data/fake_twitter_text_dict.pkl')\n",
    "infact_fake_tweet_text_dict = load(scraping_data_path+'infact_news_data/fake_twitter_text_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f75a22d-d494-41d2-808c-8f5a8ad7a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cancat(data_path, file_name):\n",
    "    \n",
    "    data_dirs = [f for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))]\n",
    "    data_list = []\n",
    "    for data_dir in data_dirs:\n",
    "        if os.path.isfile(os.path.join(data_path+data_dir, file_name)):\n",
    "            df = load(os.path.join(data_path+data_dir, file_name))\n",
    "            data_list.append(df)\n",
    "    \n",
    "    return pd.concat(data_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b9e9e5-ef35-44f9-b4b1-7d6c3aca5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fij_tweet_data = fij_tweet_data[~fij_tweet_data.duplicated('id')]\n",
    "infact_tweet_data = infact_tweet_data[~infact_tweet_data.duplicated('id')]\n",
    "nikkei_tweet_data = nikkei_tweet_data[~nikkei_tweet_data.duplicated('id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ff739e9-9e6c-4009-983f-6d90a98208b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149785\n",
      "144966\n",
      "143262\n",
      "143015\n",
      "1912\n",
      "142518\n",
      "1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/arrays/datetimelike.py:1345: PerformanceWarning: Adding/subtracting object-dtype array to DatetimeArray not vectorized\n",
      "  PerformanceWarning,\n"
     ]
    }
   ],
   "source": [
    "# fij_tweet_data['label'] = 1\n",
    "# infact_tweet_data['label'] = 1\n",
    "# nikkei_tweet_data['label'] = 0\n",
    "\n",
    "# def original_kw(value, tweet_text_dict):\n",
    "#     return [k for k, v in tweet_text_dict.items() if value in v][0]\n",
    "\n",
    "# fij_tweet_data['event'] = fij_tweet_data['kw'].apply(original_kw, tweet_text_dict = fij_fake_tweet_text_dict)\n",
    "# infact_tweet_data['event'] = infact_tweet_data['kw'].apply(original_kw, tweet_text_dict = infact_fake_tweet_text_dict)\n",
    "# nikkei_tweet_data['event'] = nikkei_tweet_data['kw']\n",
    "\n",
    "tweet_data_list = [fij_tweet_data, infact_tweet_data, nikkei_tweet_data]\n",
    "tweet_kw_list = ['日本', 'go', '2回接種済みだった', '機能停止した', '除染作業', 'と言い出した。', '日本政府は', 'PCR', '大阪の場合は', '病院に入院しているらしい', '各党の第一声', '日本初の死亡者']\n",
    "\n",
    "df_tweet_data = pd.concat(tweet_data_list, axis=0)\n",
    "print(len(df_tweet_data))\n",
    "df_tweet_data = df_tweet_data[~df_tweet_data['kw'].isin(tweet_kw_list)]\n",
    "print(len(df_tweet_data))\n",
    "df_tweet_data = df_tweet_data[~df_tweet_data.duplicated('id')]\n",
    "print(len(df_tweet_data))\n",
    "\n",
    "df_tweet_data['created_at'] = pd.to_datetime(df_tweet_data['created_at'])\n",
    "df_tweet_data = df_tweet_data[df_tweet_data['created_at']>datetime.datetime(2020, 1, 1, 0, 0, 0, tzinfo=UTC)]\n",
    "\n",
    "print(len(df_tweet_data))\n",
    "\n",
    "print(df_tweet_data['event'].nunique())\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_tweet_data['event'])\n",
    "\n",
    "df_tweet_data['event_id'] = le.transform(df_tweet_data['event'])\n",
    "\n",
    "event_cnt = df_tweet_data.groupby('event_id', as_index=False)['event'].count().rename(columns={'event':'event_cnt'})\n",
    "df_tweet_data = df_tweet_data[~df_tweet_data['event_id'].isin(event_cnt['event_cnt']<4)].copy()\n",
    "\n",
    "print(len(df_tweet_data))\n",
    "\n",
    "print(df_tweet_data['event'].nunique())\n",
    "\n",
    "def timestamps_mk(df_tweet_data):\n",
    "    for key in df_tweet_data['event_id'].unique().tolist():\n",
    "        base_time = df_tweet_data[df_tweet_data['event_id']==key].sort_values(['id'])[:1]['created_at'].tolist()[0]\n",
    "        df_tweet_data.loc[df_tweet_data['event_id']==key, 'base_time'] = base_time\n",
    "    \n",
    "    return df_tweet_data['created_at'] - df_tweet_data['base_time']\n",
    "\n",
    "df_tweet_data['timestamps'] = timestamps_mk(df_tweet_data)\n",
    "df_tweet_data['timestamps'] = df_tweet_data['timestamps'].map(lambda x: x.total_seconds())\n",
    "\n",
    "def text_extraction(text):\n",
    "    text = re.sub(r'@\\w*', '', text)\n",
    "    text = re.sub(r'https:.*', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "df_tweet_data['text'] = df_tweet_data['text'].map(text_extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49e64d9b-fcc9-420d-a03b-9db7e37308b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_dataset(df_dataset):\n",
    "    \n",
    "    datset_dict = {}\n",
    "    \n",
    "    for key in df_tweet_data['event_id'].unique().tolist():\n",
    "        messages_dict = {}\n",
    "        messages_dict['id'] = df_tweet_data[df_tweet_data['event_id']==key].sort_values(['id'])['id'].tolist()\n",
    "        messages_dict['uid'] = df_tweet_data[df_tweet_data['event_id']==key].sort_values(['id'])['author_id'].tolist()\n",
    "        messages_dict['timestamps'] = df_tweet_data[df_tweet_data['event_id']==key].sort_values(['id'])['timestamps'].tolist()\n",
    "        messages_dict['text'] = df_tweet_data[df_tweet_data['event_id']==key].sort_values(['id'])['text'].tolist()\n",
    "        messages_dict['label'] = df_tweet_data[df_tweet_data['event_id']==key]['label'].tolist()[0]\n",
    "        datset_dict[str(key)] = messages_dict\n",
    "        \n",
    "    return datset_dict\n",
    "    \n",
    "    \n",
    "def word_segmentation(text:str, replace_dict:dict, exclude_task_list:list, a):\n",
    "    \n",
    "    token_list = []\n",
    "    for token in a.analyze(text):\n",
    "        token = token.translate(str.maketrans(replace_dict)) # 邪魔な文字を除く\n",
    "        if token not in exclude_task_list and not token.isdecimal(): # 意味ないワードを除く\n",
    "            token_list.append(token)\n",
    "            token_list = [s for s in token_list if not s.startswith('#')]\n",
    "            \n",
    "    token_str = ' '.join(token_list)\n",
    "    \n",
    "    return token_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c95b5a7-cf51-4504-993f-e2b0c55226a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datset_dict = mk_dataset(df_tweet_data)\n",
    "\n",
    "replace_dict = {'[':'', ']':'', '/':'', '+':'', '(':'', ')':'', '等':'', ',':'', '.':'', '<':'', '>':'', '-':'', '?':'', ':':'', '|':''}\n",
    "exclude_task_list = ['new', 'rt', '籏智広太', 'インファクト', 'ファクトチェック', 'factcheck', 'infact', 'こび', 'ナビ']\n",
    "\n",
    "# analyzerモジュールで形態素分析\n",
    "char_filters = [UnicodeNormalizeCharFilter()]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "token_filters = [CompoundNounFilter(),\n",
    "                 POSStopFilter(['記号']),\n",
    "                 LowerCaseFilter(),\n",
    "                 ExtractAttributeFilter('surface')]\n",
    "\n",
    "a = Analyzer(char_filters=char_filters, tokenizer=tokenizer ,token_filters=token_filters)\n",
    "\n",
    "for key in datset_dict.keys():\n",
    "    text_word_segmentation_list = []\n",
    "    for text in datset_dict[key]['text']:\n",
    "        text_word_segmentation_list.append(word_segmentation(text, replace_dict, exclude_task_list, a))\n",
    "    datset_dict[key]['text'] = text_word_segmentation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45e09896-8ed4-4e23-89d5-6986e24c2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(datset_dict, dataset_path+'datset_dict.pkl')\n",
    "dump(df_tweet_data, dataset_path+'df_tweet_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd14b4-5918-4f59-a729-e36b7b9413ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
